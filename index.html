<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition">
  <meta name="keywords" content="3D Object Recognition, Point Cloud, Multimodal Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://ndkhanh360.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <!-- TODO: Add your other research projects here -->
          <a class="navbar-item" href="https://openaccess.thecvf.com/content/WACV2023/html/Le_Uncertainty-Aware_Label_Distribution_Learning_for_Facial_Expression_Recognition_WACV_2023_paper.html">
            LDLVA
          </a>
          <a class="navbar-item" href="https://link.springer.com/article/10.1007/s00521-021-06778-x">
            GLAMOR-Net
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Occlusion-aware Text-Image-Point Cloud<br>Pretraining for Open-World<br>3D Object Recognition</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ndkhanh360.github.io/">Khanh Nguyen</a>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/site/gmjally/home">Ghulam Mubashar Hassan</a>,</span>
            <span class="author-block">
              <a href="https://ajmalsaeed.net/">Ajmal Mian</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">The University of Western Australia</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><strong>[CVPR 2025]</strong></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2502.10674"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2502.10674"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <!-- TODO: Add your video link -->
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <!-- TODO: Add your code repository link -->
                <a href="https://github.com/ndkhanh360/OccTIP"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <!-- TODO: Add your dataset link if applicable -->
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Intro image  -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- TODO: Replace with your teaser video or image -->
      <img src="./static/images/fig1.png" alt="Teaser image" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        Comparison to existing methods. (a) State-of-the-art approaches pretrain 3D encoders on complete point clouds, which differ significantly from occluded ones in practical scenarios (top). This leads to a substantial gap in zero-shot performance between ModelNet40 benchmark with full point clouds and ScanObjectNN with real-world data (bottom). (b) The proposed framework OccTIP pretrains 3D models on partial point clouds to better simulate practical conditions, leading to significant improvements on various recognition tasks, especially when combined with our DuoMamba architecture. (c) Compared to the popular PointBERT, DuoMamba has significantly lower FLOPs (top) and latency (bottom) during inference, making it better suited for real-world applications.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <!-- TODO: Replace with your result videos/images -->
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/placeholder.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/placeholder.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/placeholder.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent open-world representation learning approaches have leveraged CLIP to enable zero-shot 3D object recognition. However, performance on real point clouds with occlusions still falls short due to the unrealistic pretraining settings. Additionally, these methods incur high inference costs because they rely on Transformer's attention modules. In this paper, we make two contributions to address these limitations. First, we propose occlusion-aware text-image-point cloud pretraining to reduce the training-testing domain gap. From 52K synthetic 3D objects, our framework generates nearly 630K partial point clouds for pretraining, consistently improving real-world recognition performances of existing popular 3D networks. Second, to reduce computational requirements, we introduce DuoMamba, a two-stream linear state space model tailored for point clouds. By integrating two space-filling curves with 1D convolutions, DuoMamba effectively models spatial dependencies between point tokens, offering a powerful alternative to Transformer. When pretrained with our framework, DuoMamba surpasses current state-of-the-art methods while reducing latency and FLOPs, highlighting the potential of our approach for real-world applications.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Proposed Method</h2>

        <!-- Pretraining framework. -->
        <h3 class="title is-4">Pretraining Framework</h3>
        <div class="content has-text-justified">
          <p>
            (a) Given a 3D object, we generate RGB and depth images from preset camera positions, which are used to construct partial point clouds. Texts are generated from dataset metadata, image captioning models, and retrieved descriptions of similar photos from LION-5B. 
          </p>
          <p>
            (b) During pretraining, we extract multi-modal features using a learnable point cloud network and frozen CLIP encoders, then align them through contrastive learning.
          </p>          
        </div>
        <div class="columns is-centered">
          <div class="column is-full has-text-centered">
            <img src="./static/images/fig2_occtip.png"
                 alt="OccTIP Pretraining Framework"
                 style="max-width: 80%; margin: 0 auto;"/>
          </div>
        </div>
        <br/>
        <!--/ Pretraining framework. -->

        <!-- DuoMamba -->
        <h3 class="title is-4">DuoMamba</h3>
        <div class="content has-text-justified">
          <p>
            In our DuoMamba block, we integrate two Hilbert curves and standard 1D convolutions with linear-time S6 modules to efficiently model geometric dependencies and enrich spatial context.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-full has-text-centered">
            <img src="./static/images/fig3_duomamba.png"
                 alt="DuoMamba"
                 style="max-width: 80%; margin: 0 auto;"/>
          </div>
        </div>
        <!--/ DuoMamba. -->

      </div>
    </div>

      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <!-- TODO: Update with your paper's citation -->
    <pre><code>@article{nguyen2025occlusion,
      title={Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition},
      author={Nguyen, Khanh and Hassan, Ghulam Mubashar and Mian, Ajmal},
      journal={CVPR},
      year={2025}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2502.10674">
        <i class="fas fa-file-pdf"></i>
      </a>
      <!-- TODO: Replace with your GitHub link -->
      <a class="icon-link" href="#" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            We thank the authors of <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> that kindly open sourced the template of this website.</p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
